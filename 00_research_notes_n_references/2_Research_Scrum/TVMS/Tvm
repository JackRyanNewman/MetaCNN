2. Graph-Level Optimizations (beyond compiler tricks)

This is what youâ€™re sensing as the â€œmystery sauceâ€ frameworks add. Itâ€™s not just math â†’ itâ€™s dataflow orchestration across the whole network.

Examples:

ğŸ”¸ a) Operator Fusion

Combine multiple elementwise ops with the convolution.

Ex: Conv â†’ BatchNorm â†’ ReLU becomes one kernel.

Saves memory bandwidth (only one store/load).

ğŸ”¸ b) Constant Folding

Pre-compute anything static.

Ex: BatchNorm parameters (scale + shift) can be folded into convolution weights at compile time.

Removes the BN op entirely during inference.

ğŸ”¸ c) Memory Reuse / Lifetime Analysis

Instead of allocating new memory for every intermediate tensor, the compiler analyzes when tensors die and reuses the same buffer.

This can shrink memory use by 10Ã— in deep nets.

General C compilers donâ€™t know tensor lifetimes across function boundaries â†’ ML compilers do.

ğŸ”¸ d) Layout Transformations

Decide if tensors should be stored as NCHW (channels-first) or NHWC (channels-last).

On CPUs: vectorization prefers NHWC.

On GPUs: coalesced memory prefers NCHW.

ML compilers insert layout transforms automatically where needed.

ğŸ”¸ e) Op Reordering

Safe reorderings like: (A + B) + C â†’ A + (B + C) to fit better into SIMD tiles.

Or reordering convolution + pooling layers for cache alignment.

ğŸ”¸ f) Quantization + Mixed Precision

Automatically replace float32 with float16/int8 if accuracy stays within tolerance.

This reduces memory + doubles throughput.

Compilers can insert cast ops where needed and fuse them away.

ğŸ”¸ g) Autotuning / Kernel Search

Run microbenchmarks on your actual hardware.

Try different tile sizes, loop orderings, vector widths.

Pick the fastest one.

TVM, cuDNN, TensorRT all do this â†’ something no static C compiler will ever attempt.

ğŸ”¸ h) Cross-Layer Scheduling

Overlap compute with data transfer.

E.g., while one layer computes on GPU, preload the next layerâ€™s weights.

This turns the whole net into a pipelined schedule.